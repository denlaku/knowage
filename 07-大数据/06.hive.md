2：解压
3：配置hive的环境变量
	在当前用户的.bashrc中配置如下内容
	export HIVE_HOME=/home/hadoop/bd/apache-hive-2.1.0-bin

4:配置hive安装目录下的conf目录下的hive-env.sh文件
	该文件可以通过复制hive-env.sh.template改名得来
	配置内容如下：
	# Set HADOOP_HOME to point to a specific hadoop install directory
	 HADOOP_HOME=/home/hadoop/bd/hadoop-2.7.3

	# Hive Configuration Directory can be controlled by:
 	export HIVE_CONF_DIR=/home/hadoop/bd/apache-hive-2.1.0-bin/conf

	# Folder containing extra ibraries required for hive compilation/execution can be controlled by:
 	export HIVE_AUX_JARS_PATH=/home/hadoop/bd/apache-hive-2.1.0-bin/lib

5：修改hive的日志文件存放的地址
	cp hive-log4j2.properties.template hive-log4j2.properties
	通过vi修改日志的存放文件
	property.hive.log.dir = /home/hadoop/bd/apache-hive-2.1.0-bin/logs

1：复制文件hive-default.xml.template更名为hive-site.xml
	cp hive-default.xml.template hive-site.xml

2：清空hive-site.xml里面的配置信息
	添加我们自定义的信息

```xml
<configuration>
    <property>
        <name>javax.jdo.option.ConnectionURL</name>
        <value>jdbc:mysql://hm02:3306/hive?createDatabaseIfNotExist=true</value>
    </property>
    <property>
        <name>javax.jdo.option.ConnectionDriverName</name>
        <value>com.mysql.jdbc.Driver</value>
    </property>
    <property>
        <name>javax.jdo.option.ConnectionUserName</name>
        <value>root</value>
    </property>
    <property>
        <name>javax.jdo.option.ConnectionPassword</name>
        <value>123</value>
    </property>
</configuration>
```



6：启动hadoop集群

7：安装默认的derby数据库为hive的元数据库
	可以先通过./schematool --help 命令来查看schematool命令的一些选项
	 ./schematool -dbType derby -initSchema，使用这个命令来安装derby数据库为元数据

8：执行bin目录下的hive命令，进入hive命令行
	./hive

1：创建表
	create table 表名
	指定分隔符创建表：create table teacher (id int, name string) row format delimited fields terminated by '\t';