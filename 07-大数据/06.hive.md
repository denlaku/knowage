## hive安装

### 1、解压

### 2、配置hive的环境变量

```shell
vi .bash_profile
export HIVE_HOME=/home/hadoop/hive-2.1.1
```

### 3、 配置hive-env.sh文件

`hive-env.sh`文件可以通过复制`hive-env.sh.template`得到

配置内容如下：

```shell
HADOOP_HOME=/home/hadoop/bd/hadoop-2.7.3
export HIVE_CONF_DIR=/home/hadoop/bd/apache-hive-2.1.0-bin/conf
export HIVE_AUX_JARS_PATH=/home/hadoop/bd/apache-hive-2.1.0-bin/lib
```

### 5、配置hive-log4j2.properties文件

```shell
# 通过复制创建hive-log4j2.properties
cp hive-log4j2.properties.template hive-log4j2.properties
# 通过vi修改日志的存放文件
property.hive.log.dir=/home/hadoop/bd/apache-hive-2.1.0-bin/logs
```

### 6、启动hadoop集群

hive是基于hadoop运行的，在启动hive之前必须启动hadoop

### 7、初始化元数据

查看schematool命令的一些选项：`./schematool --help`

这里可以采用derby数据库，也可以使用MySql数据库

#### derby数据库

derby数据库为hive的默认的元数据库，元数据初始化命令：

`./schematool -dbType derby -initSchema`

#### MySql数据库

使用mysql的话，需要创建hive-site.xml文件，添加我们自定义的信息

```xml
<configuration>
    <property>
        <name>javax.jdo.option.ConnectionURL</name>
        <value>jdbc:mysql://hm02:3306/hive?createDatabaseIfNotExist=true</value>
    </property>
    <property>
        <name>javax.jdo.option.ConnectionDriverName</name>
        <value>com.mysql.jdbc.Driver</value>
    </property>
    <property>
        <name>javax.jdo.option.ConnectionUserName</name>
        <value>root</value>
    </property>
    <property>
        <name>javax.jdo.option.ConnectionPassword</name>
        <value>123</value>
    </property>
</configuration>
```

在mysql中初始化元数据

`./schematool -dbType mysql -initSchema`

关于mysql作为元数据库的几点说明
	1）hive当中创建的表的信息，在元数据库的TBLS表里面
	2）这个表的字段信息，在元数据库的COLUMNS_V2表里面
	3）这个表在HDFS上面的位置信息，在元数据库的SDS表里面

## hive使用

### 1：创建表

```sql
-- 创建表
create table teacher (
    id int, name string
) 
row format delimited fields terminated by '\t'; -- 指定分隔符
-------------------------------------------------------------------------------------------------
-- 创建外部表
create external table person2(
    id int,
    name string,
    hobby array<string>,
    addr map<string,string>
)
row format delimited 
fields terminated by ',' 
collection items terminated by '-' 
map keys terminated by ':' 
location '/user/person2'；

/*
内部表(MANAGED TABLE): 未被external修改的
外部表(EXTERNAL TABLE): 被external修饰的
内部表和外部表区别
  1）内部表数据由hive自身管理，外部表数据由hdfs来管理
    内部表数据存储的位置默认/user/hive/warehouse,
    外部表数据存储的位置由用户自己指定
  2）删除内部表会直接删除元数据和存储数据
    删除外部表仅仅只会删除元数据，HDFS上的文件不会删除。
*/
-------------------------------------------------------------------------------------------------
-- 分区表
create table p1(
	id int,
	name string,
	hobby array<string>,
	addr map<string,string>
)
partitioned by (p_dt string) -- 指定分区字段
row format delimited 
fields terminated by ',' 
collection items terminated by '-' 
map keys terminated by ':';
-- 注意：分区字段不能和表中的字段重复，若要创建分区表，必须在表定义的时候创建partition
```

### 2、加载文件中的数据至hive表中

```
load data local inpath '/home/hadoop/teacher.txt' into table teacher;
load data local inpath '/home/hadoop/data.txt' into table person2;
load data local inpath '/home/hadoop/data.txt' into table p1 partition(p_dt='201808');
```

表分区

```shell
# 查看p1表的表分区
show partitions p1; 
# 添加表分区
alter table p1 add partition (p_dt=201809);
# 删除分区
alter table p1 drop partition (p_dt=201809);

```



### 3、删除表

```sql
drop table tableName;
```

### 4、删除表数据

```sql
truncate table tableName;
```

